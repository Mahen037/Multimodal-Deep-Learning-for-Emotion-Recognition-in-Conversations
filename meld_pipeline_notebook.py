# -*- coding: utf-8 -*-
"""meld_pipeline_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wf2Tjk5cA-Gi0yGqL8fbr4fman5AilU9

# MELD Pipeline (Feature Extraction → Training → Eval)

**Workflow**
1. Install dependencies.
2. Configure paths/models/hyperparameters.
3. Extract features (run once).
4. Train unimodal or multimodal models.
5. Evaluate and plot results.
"""

# Commented out IPython magic to ensure Python compatibility.
# Optional: install dependencies on a fresh runtime
# %pip install -q torch torchaudio torchvision transformers pandas numpy scikit-learn tqdm opencv-python-headless matplotlib soundfile

from google.colab import drive
drive.mount("/content/drive", force_remount=True)


PROJECT_ROOT = "/content/drive/MyDrive/FinalProject-498K"
PIPE_DIR = f"{PROJECT_ROOT}/mahendra-experiments"

import os
os.environ["MELD_DATASET_ROOT"] = f"{PROJECT_ROOT}/MELD-RAW/MELD.Raw"
os.environ["MELD_FEATURES_ROOT"] = f"{PROJECT_ROOT}/MELD-Features-Models-498K/MELD.Features.Models"


import sys
sys.path.append(PIPE_DIR)

import importlib

import meld_pipeline as mp
mp = importlib.reload(mp)

# where is the module coming from?
print("Using meld_pipeline at:", mp.__file__)

Paths = mp.Paths
ModelNames = mp.ModelNames
TrainConfig = mp.TrainConfig
run_feature_extraction = mp.run_feature_extraction
train_unimodal = mp.train_unimodal
train_multimodal = mp.train_multimodal
eval_unimodal_model = mp.eval_unimodal_model
eval_multimodal_model = mp.eval_multimodal_model
plot_f1_bar = mp.plot_f1_bar
plot_confusion = mp.plot_confusion
device = mp.device

# Point dataset_root to the directory that contains MELD-RAW/ (defaults to current project root).
# You can also set the env vars MELD_DATASET_ROOT and MELD_FEATURES_ROOT before starting the kernel.
paths = Paths()
train_cfg = TrainConfig()
model_names = ModelNames()

print(paths)
print(train_cfg)
print(model_names)
print("Using device:", device)

import torch.optim as optim

# Hyperparameters (override defaults from meld_pipeline.py)
# Unimodal
UNI_HIDDEN = 512
UNI_DROPOUT = 0.2
UNI_LR = 5e-5 # learning rate
UNI_WD = 1e-2 # weight decay
UNI_EPOCHS = 15
UNI_OPTIMIZER = optim.AdamW
UNI_OPT_KWARGS = {"lr": UNI_LR, "weight_decay": UNI_WD}

# Multimodal
MM_HIDDEN = 512
MM_HEADS = 4
MM_LAYERS = 2
MM_DROPOUT = 0.2
MM_LR = 5e-5 # learning rate
MM_WD = 1e-2 # weight decay
MM_W_TEXT = 0.5 # weight for text modality
MM_W_AUDIO = 0.3 # weight for audio modality
MM_W_VIDEO = 0.2 # weight for video modality
MM_EPOCHS = 15 # number of epochs
MM_OPTIMIZER = optim.AdamW # optimizer
MM_OPT_KWARGS = {"lr": MM_LR, "weight_decay": MM_WD} # optimizer arguments

# Shared
BATCH_SIZE = 32 # batch size

# Apply batch size to TrainConfig for loaders that use it elsewhere
train_cfg.batch_size = BATCH_SIZE

# 1) Feature extraction (run once; creates features under paths.features_root)
DO_EXTRACT = False  # change to True the first time

if DO_EXTRACT:
    run_feature_extraction(paths=paths, model_names=model_names, train_cfg=train_cfg)

# 2) Train unimodal baselines (text/audio/video)
DO_TRAIN_UNIMODAL = True

if DO_TRAIN_UNIMODAL:
    for modality in ["text", "audio", "video"]:
        train_unimodal(
            modality,
            hidden=UNI_HIDDEN,
            dropout=UNI_DROPOUT,
            lr=UNI_LR,
            weight_decay=UNI_WD,
            num_epochs=UNI_EPOCHS,
            batch_size=BATCH_SIZE,
            optimizer_cls=UNI_OPTIMIZER,
            optimizer_kwargs=UNI_OPT_KWARGS,
        )

# 3) Train multimodal models
DO_TRAIN_MM_EARLY = True

if DO_TRAIN_MM_EARLY:
    train_multimodal(
        "early",
        hidden_dim=MM_HIDDEN,
        num_heads=MM_HEADS,
        num_layers=MM_LAYERS,
        dropout=MM_DROPOUT,
        lr=MM_LR,
        weight_decay=MM_WD,
        num_epochs=MM_EPOCHS,
        batch_size=BATCH_SIZE,
        optimizer_cls=MM_OPTIMIZER,
        optimizer_kwargs=MM_OPT_KWARGS,
    )

DO_TRAIN_MM_LATE = True

if DO_TRAIN_MM_LATE:
    train_multimodal(
        "late",
        hidden_dim=MM_HIDDEN,
        num_heads=MM_HEADS,
        num_layers=MM_LAYERS,
        dropout=MM_DROPOUT,
        lr=MM_LR,
        weight_decay=MM_WD,
        num_epochs=MM_EPOCHS,
        batch_size=BATCH_SIZE,
        w_text=MM_W_TEXT,
        w_audio=MM_W_AUDIO,
        w_video=MM_W_VIDEO,
        learnable_weights=True,
        optimizer_cls=MM_OPTIMIZER,
        optimizer_kwargs=MM_OPT_KWARGS,
    )

# 4) Evaluate saved models and plot results
DO_PLOT_EVAL = True
EVAL_SPLIT = "test"  # change to "dev" for dev numbers

if DO_PLOT_EVAL:
    results = {}

    # Unimodal models
    try:
        f1_t, acc_t, lab_t, pred_t = eval_unimodal_model(
            "text", f"{PROJECT_ROOT}/ckpts/unimodal_text_best.pt", split=EVAL_SPLIT
        )
        results["uni_text"] = f1_t
        print(f"Unimodal text ({EVAL_SPLIT}) F1: {f1_t:.4f}, Acc: {acc_t:.4f}")
    except FileNotFoundError as e:
        print("Text model missing:", e)

    try:
        f1_a, acc_a, lab_a, pred_a = eval_unimodal_model(
            "audio", f"{PROJECT_ROOT}/ckpts/unimodal_audio_best.pt", split=EVAL_SPLIT
        )
        results["uni_audio"] = f1_a
        print(f"Unimodal audio ({EVAL_SPLIT}) F1: {f1_a:.4f}, Acc: {acc_a:.4f}")
    except FileNotFoundError as e:
        print("Audio model missing:", e)

    try:
        f1_v, acc_v, lab_v, pred_v = eval_unimodal_model(
            "video", f"{PROJECT_ROOT}/ckpts/unimodal_video_best.pt", split=EVAL_SPLIT
        )
        results["uni_video"] = f1_v
        print(f"Unimodal video ({EVAL_SPLIT}) F1: {f1_v:.4f}, Acc: {acc_v:.4f}")
    except FileNotFoundError as e:
        print("Video model missing:", e)

    # Multimodal models (set hyperparams to match training)
    try:
        f1_early, acc_early, lab_early, pred_early = eval_multimodal_model(
            "early",
            f"{PROJECT_ROOT}/ckpts/multimodal_early_fusion_best.pt",
            split=EVAL_SPLIT,
            hidden_dim=512,
            num_heads=4,
            num_layers=2,
            dropout=0.2,
        )
        results["mm_early"] = f1_early
        print(f"Multimodal early-fusion ({EVAL_SPLIT}) F1: {f1_early:.4f}, Acc: {acc_early:.4f}")
    except FileNotFoundError as e:
        print("Early-fusion model missing:", e)

    try:
        f1_late, acc_late, lab_late, pred_late = eval_multimodal_model(
            "late",
            f"{PROJECT_ROOT}/ckpts/multimodal_late_fusion_best.pt",
            split=EVAL_SPLIT,
            hidden_dim=512,
            num_heads=4,
            num_layers=2,
            dropout=0.2,
        )
        results["mm_late"] = f1_late
        print(f"Multimodal late-fusion ({EVAL_SPLIT}) F1: {f1_late:.4f}, Acc: {acc_late:.4f}")
    except FileNotFoundError as e:
        print("Late-fusion model missing:", e)

    if results:
      plot_f1_bar(results, f"MELD {EVAL_SPLIT.title()} Weighted F1 (Unimodal vs Multimodal)")
      if "mm_late" in results:
          plot_confusion(lab_late, pred_late, f"Late-fusion multimodal ({EVAL_SPLIT})")
      elif "mm_early" in results:
          plot_confusion(lab_early, pred_early, f"Early-fusion multimodal ({EVAL_SPLIT})")
      elif "uni_text" in results:
          plot_confusion(lab_t, pred_t, f"Unimodal text ({EVAL_SPLIT})")

import pandas as pd
from collections import Counter
import meld_pipeline as mp

splits = ["train", "dev", "test"]
for split in splits:
    csv_path = mp.find_meld_csv(mp.paths.dataset_root, split)
    df = pd.read_csv(csv_path)
    labels = df["Emotion"].str.lower()
    counts = labels.value_counts()
    ordered = [counts.get(lbl, 0) for lbl in mp.EMOTIONS]
    print(f"\n{split.upper()} size: {len(df)}")
    for lbl, cnt in zip(mp.EMOTIONS, ordered):
        print(f"  {lbl:9s}: {cnt}")