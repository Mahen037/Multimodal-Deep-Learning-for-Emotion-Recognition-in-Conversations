# Multimodal Deep Learning for Emotion Recognition in Conversations

This repository contains code and experimental artifacts for multimodal emotion recognition on the **MELD** dataset using text, audio, and visual modalities.

---

## üìÅ Files


### File Description

- **`meld_pipeline.py`**  
  Python script for training and evaluating the multimodal models.

- **`meld_pipeline_notebook.ipynb`**  
  Jupyter notebook of the pipeline. Was run using **Google Colab**.

- **`meld_pipeline_notebook.html`**  
  Static HTML version of the notebook with **all outputs preserved**.  
  It's for quickly viewing results on GitHub.

---

## ‚ñ∂Ô∏è Usage

- **View results:** Open `meld_pipeline_notebook.html`


---

## üìå Note
GitHub may not render the notebook preview correctly due to Colab widget metadata.  
The HTML file provides a complete and reliable view of all results.
